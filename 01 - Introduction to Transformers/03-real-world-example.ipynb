{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to mokeypatch torchtext to get this to work\n",
    "# The reason is that dated torchtext had bad dataset hash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "import GPUtil\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from transformer.training import Batch, TrainState, run_epoch, create_dataloaders\n",
    "from transformer.transformer import make_model, greedy_decode\n",
    "from transformer.tokenizers import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de, spacy_en = show_example(load_tokenizers)\n",
    "vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None\n",
    "        \n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    #? why would / by norm be needed and then * by norm?\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        # 1st term is used for reporting, 2nd term is used for backprop\n",
    "        return sloss.data * norm, sloss\n",
    "    \n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = torch.zeros_like(x.data)\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        # https://yuyangyy.medium.com/understand-torch-scatter-b0fd6275331c\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for LambdaLR function\n",
    "# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html\n",
    "\n",
    "# This regulates learning for each step\n",
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(\n",
    "    gpu,\n",
    "    ngpus_per_node,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    config,\n",
    "    is_distributed=False,\n",
    "):\n",
    "    print(f\"Train worker process using GPU: {gpu} for training\", flush=True)\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "    pad_idx = vocab_tgt[\"<blank>\"]\n",
    "    d_model = 512\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    model.cuda(gpu)\n",
    "    module = model\n",
    "    is_main_process = True\n",
    "    if is_distributed:\n",
    "        dist.init_process_group(\n",
    "            \"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node\n",
    "        )\n",
    "        model = DDP(model, device_ids=[gpu])\n",
    "        module = model.module\n",
    "        is_main_process = gpu == 0\n",
    "\n",
    "    criterion = LabelSmoothing(\n",
    "        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n",
    "    )\n",
    "    criterion.cuda(gpu)\n",
    "\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(\n",
    "        gpu,\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=config[\"batch_size\"] // ngpus_per_node,\n",
    "        max_padding=config[\"max_padding\"],\n",
    "        is_distributed=is_distributed,\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "    train_state = TrainState()\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        if is_distributed:\n",
    "            train_dataloader.sampler.set_epoch(epoch)\n",
    "            valid_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        model.train()\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n",
    "        _, train_state = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config[\"accum_iter\"],\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        GPUtil.showUtilization()\n",
    "        if is_main_process:\n",
    "            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n",
    "            torch.save(module.state_dict(), file_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n",
    "        model.eval()\n",
    "        sloss = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )\n",
    "        print(sloss)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if is_main_process:\n",
    "        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "        torch.save(module.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distributed_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n",
    "\n",
    "    ngpus = torch.cuda.device_count()\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12356\"\n",
    "    print(f\"Number of GPUs detected: {ngpus}\")\n",
    "    print(\"Spawning training processes ...\")\n",
    "    mp.spawn(\n",
    "        train_worker,\n",
    "        nprocs=ngpus,\n",
    "        args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n",
    "    if config[\"distributed\"]:\n",
    "        train_distributed_model(\n",
    "            vocab_src, vocab_tgt, spacy_de, spacy_en, config\n",
    "        )\n",
    "    else:\n",
    "        train_worker(\n",
    "            0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False\n",
    "        )\n",
    "\n",
    "\n",
    "def load_trained_model():\n",
    "    config = {\n",
    "        \"batch_size\": 32,\n",
    "        \"distributed\": False,\n",
    "        \"num_epochs\": 8,\n",
    "        \"accum_iter\": 10,\n",
    "        \"base_lr\": 1.0,\n",
    "        \"max_padding\": 72,\n",
    "        \"warmup\": 3000,\n",
    "        \"file_prefix\": \"multi30k_model_\",\n",
    "    }\n",
    "    model_path = \"multi30k_model_final.pt\"\n",
    "    if not exists(model_path):\n",
    "        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\n",
    "\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3878d7a7a447c7cc89f75f8ff38bbdbae1c418e3499f30fae1ec67e71de8e5b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
