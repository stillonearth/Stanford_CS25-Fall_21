{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from transformer.training import Batch, run_epoch\n",
    "from transformer.transformer import make_model, greedy_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None\n",
    "        \n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will generate a random batch of data where src and \n",
    "# tgt are the same sequence of random integers\n",
    "\n",
    "def data_gen(V, batch_size, nbatches, device):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batch_size, 10))\n",
    "        data[:, 0] = 1\n",
    "        src = data.requires_grad_(False).clone().detach().to(device)\n",
    "        tgt = data.requires_grad_(False).clone().detach().to(device)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    #? why would / by norm be needed and then * by norm?\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        # 1st term is used for reporting, 2nd term is used for backprop\n",
    "        return sloss.data * norm, sloss\n",
    "    \n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = torch.zeros_like(x.data)\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        # https://yuyangyy.medium.com/understand-torch-scatter-b0fd6275331c\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for LambdaLR function\n",
    "# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html\n",
    "\n",
    "# This regulates learning for each step\n",
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_simple_model(device=\"cuda\"):\n",
    "    V = 11 # source and target vocab size \n",
    "    # Use criterion that scatters uncertainty (1-p) over all labels before applying KLDivLoss\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0).to(device)\n",
    "    # initialize Transformer model\n",
    "    model = make_model(V, V, N=2).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    batch_size = 80\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 20, device),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train\",\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "    model.eval()\n",
    "    src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]).to(device)\n",
    "    max_len = src.shape[1]\n",
    "    src_mask = torch.ones(1, 1, max_len).to(device)\n",
    "    # greedy_decode is one stratery of unrolling transformer prediction\n",
    "    print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   3.02 | Tokens / Sec:   624.4 | Learning Rate: 5.5e-06\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   2.01 | Tokens / Sec: 16720.5 | Learning Rate: 6.1e-05\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.75 | Tokens / Sec: 15633.4 | Learning Rate: 1.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.43 | Tokens / Sec: 12822.0 | Learning Rate: 1.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.04 | Tokens / Sec: 13724.0 | Learning Rate: 2.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.59 | Tokens / Sec: 16807.5 | Learning Rate: 2.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.36 | Tokens / Sec: 16314.8 | Learning Rate: 3.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.21 | Tokens / Sec: 16987.2 | Learning Rate: 3.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.20 | Tokens / Sec: 17218.1 | Learning Rate: 4.5e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.14 | Tokens / Sec: 17116.1 | Learning Rate: 5.0e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.21 | Tokens / Sec: 13313.8 | Learning Rate: 5.6e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.10 | Tokens / Sec: 14961.0 | Learning Rate: 6.1e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.12 | Tokens / Sec: 16521.5 | Learning Rate: 6.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.12 | Tokens / Sec: 16361.9 | Learning Rate: 7.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.14 | Tokens / Sec: 17520.6 | Learning Rate: 7.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.13 | Tokens / Sec: 14996.8 | Learning Rate: 8.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec: 13499.8 | Learning Rate: 8.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.13 | Tokens / Sec: 12596.4 | Learning Rate: 9.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec: 10987.7 | Learning Rate: 1.0e-03\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.08 | Tokens / Sec: 15019.7 | Learning Rate: 1.1e-03\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'greedy_decode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Model would learn to copy the input sequence\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m execute_example(example_simple_model)\n",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m, in \u001b[0;36mexecute_example\u001b[1;34m(fn, args)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute_example\u001b[39m(fn, args\u001b[39m=\u001b[39m[]):\n\u001b[0;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m RUN_EXAMPLES:\n\u001b[1;32m---> 19\u001b[0m         fn(\u001b[39m*\u001b[39;49margs)\n",
      "Cell \u001b[1;32mIn[8], line 36\u001b[0m, in \u001b[0;36mexample_simple_model\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m     34\u001b[0m src_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, max_len)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     35\u001b[0m \u001b[39m# greedy_decode is one stratery of unrolling transformer prediction\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[39mprint\u001b[39m(greedy_decode(model, src, src_mask, max_len\u001b[39m=\u001b[39mmax_len, start_symbol\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'greedy_decode' is not defined"
     ]
    }
   ],
   "source": [
    "# Model would learn to copy the input sequence\n",
    "execute_example(example_simple_model)\n",
    "\n",
    "# cuda Tokens / Sec on 2080Ti:  1.35e+04, cpu: 700\n",
    "# 19 times faster on GPU than CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ea6adb180b12ed72836e614d5d57295654ca2a9780d621124b81b6a9baa809"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
