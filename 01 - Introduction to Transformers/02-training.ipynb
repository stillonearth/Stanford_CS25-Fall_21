{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from transformer.training import Batch, run_epoch\n",
    "from transformer.transformer import make_model, greedy_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None\n",
    "        \n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will generate a random batch of data where src and \n",
    "# tgt are the same sequence of random integers\n",
    "\n",
    "def data_gen(V, batch_size, nbatches, device):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batch_size, 10))\n",
    "        data[:, 0] = 1\n",
    "        src = data.requires_grad_(False).clone().detach().to(device)\n",
    "        tgt = data.requires_grad_(False).clone().detach().to(device)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    #? why would / by norm be needed and then * by norm?\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        # 1st term is used for reporting, 2nd term is used for backprop\n",
    "        return sloss.data * norm, sloss\n",
    "    \n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = torch.zeros_like(x.data)\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        # https://yuyangyy.medium.com/understand-torch-scatter-b0fd6275331c\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for LambdaLR function\n",
    "# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html\n",
    "\n",
    "# This regulates learning for each step\n",
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_simple_model(device=\"cuda\"):\n",
    "    V = 11 # source and target vocab size \n",
    "    # Use criterion that scatters uncertainty (1-p) over all labels before applying KLDivLoss\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0).to(device)\n",
    "    # initialize Transformer model\n",
    "    model = make_model(V, V, N=2).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    batch_size = 80\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 20, device),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train\",\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "    model.eval()\n",
    "    src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]).to(device)\n",
    "    max_len = src.shape[1]\n",
    "    src_mask = torch.ones(1, 1, max_len).to(device)\n",
    "    # greedy_decode is one stratery of unrolling transformer prediction\n",
    "    print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   3.31 | Tokens / Sec: 12697.8 | Learning Rate: 5.5e-06\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   2.17 | Tokens / Sec: 16463.9 | Learning Rate: 6.1e-05\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.76 | Tokens / Sec: 16471.2 | Learning Rate: 1.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.50 | Tokens / Sec: 17072.0 | Learning Rate: 1.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.03 | Tokens / Sec: 14871.3 | Learning Rate: 2.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.73 | Tokens / Sec: 15679.4 | Learning Rate: 2.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.38 | Tokens / Sec: 16426.5 | Learning Rate: 3.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.21 | Tokens / Sec: 15791.9 | Learning Rate: 3.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.20 | Tokens / Sec: 14971.4 | Learning Rate: 4.5e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.16 | Tokens / Sec: 13785.2 | Learning Rate: 5.0e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.16 | Tokens / Sec: 15433.0 | Learning Rate: 5.6e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.17 | Tokens / Sec: 16502.0 | Learning Rate: 6.1e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.10 | Tokens / Sec: 15594.7 | Learning Rate: 6.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.13 | Tokens / Sec: 14810.4 | Learning Rate: 7.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.17 | Tokens / Sec: 16224.6 | Learning Rate: 7.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.10 | Tokens / Sec: 14302.3 | Learning Rate: 8.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.22 | Tokens / Sec: 17118.2 | Learning Rate: 8.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.16 | Tokens / Sec: 14872.4 | Learning Rate: 9.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.10 | Tokens / Sec: 14221.8 | Learning Rate: 1.0e-03\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.09 | Tokens / Sec: 15706.6 | Learning Rate: 1.1e-03\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Model would learn to copy the input sequence\n",
    "execute_example(example_simple_model)\n",
    "\n",
    "# cuda Tokens / Sec on 2080Ti:  1.35e+04, cpu: 700\n",
    "# 19 times faster on GPU than CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ea6adb180b12ed72836e614d5d57295654ca2a9780d621124b81b6a9baa809"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
